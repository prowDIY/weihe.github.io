<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Gang Niu (Research Scientist at RIKEN-AIP)</title>
    <base href="https://niug1984.github.io/publication.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Gang Niu (Research Scientist at RIKEN-AIP)</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="publication.html" class="current">Publications</a></div>
    <div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>
<td id="layout-content">

    <h1 style="margin-top: 0em">Publications</h1><br>
    <p>[ <a href="#preprint">Preprints</a>,
         <a href="#conference">Conference Papers</a>,
         <a href="#journal">Journal Articles</a>,
         <a href="#workshop">Workshop Presentations</a>,
         <a href="#thesis">Theses</a> ]</p>
    <p>An asterisk (*) beside authors' names indicates equal contributions.</p>

    <div>
        <h2><hr><a name="preprint"></a>Preprints</h2>
        <ol>
            <li><p>
                Y. Bai, E. Yang, B. Han, Y. Yang, J. Li, Y. Mao, G. Niu, and T. Liu.<br>
                Understanding and improving early stopping for learning with noisy labels.<br>
                [ <a href="http://arxiv.org/abs/2106.15853" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                R. Gao, F. Liu, K. Zhou, G. Niu, B. Han, and J. Cheng.<br>
                Local reweighting for adversarial training.<br>
                [ <a href="http://arxiv.org/abs/2106.15776" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Y. Cao, L. Feng, S. Shu, Y. Xu, B. An, G. Niu, and M. Sugiyama.<br>
                Multi-class classification from single-class data with confidences.<br>
                [ <a href="http://arxiv.org/abs/2106.08864" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Q. Wang, F. Liu, B. Han, T. Liu, C. Gong, G. Niu, M. Zhou, and M. Sugiyama.<br>
                Probabilistic margins for instance reweighting in adversarial training.<br>
                [ <a href="http://arxiv.org/abs/2106.07904" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Y. Zhang, M. Gong, T. Liu, G. Niu, X. Tian, B. Han, B. Sch√∂lkopf, and K. Zhang.<br>
                Adversarial robustness through the lens of causality.<br>
                [ <a href="http://arxiv.org/abs/2106.06196" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Lv, L. Feng, M. Xu, B. An, G. Niu, X. Geng, and M. Sugiyama.<br>
                On the robustness of average losses for partial-label learning.<br>
                [ <a href="http://arxiv.org/abs/2106.06152" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Zhu, J. Yao, B. Han, J. Zhang, T. Liu, G. Niu, J. Zhou, J. Xu, and H. Yang.<br>
                Reliable adversarial distillation with unreliable teachers.<br>
                [ <a href="http://arxiv.org/abs/2106.04928" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                X. Xia, T. Liu, B. Han, M. Gong, J. Yu, G. Niu, and M. Sugiyama.<br>
                Instance correction for learning with open-set noisy labels.<br>
                [ <a href="http://arxiv.org/abs/2106.00455" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                X. Xia, T. Liu, B. Han, M. Gong, J. Yu, G. Niu, and M. Sugiyama.<br>
                Sample selection with uncertainty of losses for learning with noisy labels.<br>
                [ <a href="http://arxiv.org/abs/2106.00445" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Wei, H. Liu, T. Liu, G. Niu, and Y. Liu.<br>
                Understanding (generalized) label smoothing when learning with noisy labels.<br>
                [ <a href="http://arxiv.org/abs/2106.04149" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Zhang*, X. Xu*, B. Han, T. Liu, G. Niu, L. Cui, and M. Sugiyama.<br>
                NoiLIn: Do noisy labels always hurt adversarial training?<br>
                [ <a href="http://arxiv.org/abs/2105.14676" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                S. Yang, E. Yang, B. Han, Y. Liu, M. Xu, G. Niu, and T. Liu.<br>
                Estimating instance-dependent label-noise transition matrix using DNNs.<br>
                [ <a href="http://arxiv.org/abs/2105.13001" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                C. Chen*, J. Zhang*, X. Xu, T. Hu, G. Niu, G. Chen, and M. Sugiyama.<br>
                Guided interpolation for adversarial training.<br>
                [ <a href="http://arxiv.org/abs/2102.07327" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                H. Chi*, F. Liu*, W. Yang, L. Lan, T. Liu, G. Niu, and B. Han.<br>
                Meta discovery: Learning to discover novel classes given very limited data.<br>
                [ <a href="http://arxiv.org/abs/2102.04002" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Zhu*, J. Zhang*, B. Han, T. Liu, G. Niu, H. Yang, M. Kankanhalli, and M. Sugiyama.<br>
                Understanding the interaction of adversarial training with noisy labels.<br>
                [ <a href="http://arxiv.org/abs/2102.03482" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                S. Wu*, X. Xia*, T. Liu, B. Han, M. Gong, N. Wang, H. Liu, and G. Niu.<br>
                Multi-class classification from noisy-similarity-labeled data.<br>
                [ <a href="http://arxiv.org/abs/2002.06508" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Y. Yao, T. Liu, B. Han, M. Gong, G. Niu, M. Sugiyama, and D. Tao.<br>
                Towards mixture proportion estimation without irreducibility.<br>
                [ <a href="http://arxiv.org/abs/2002.03673" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Zhang*, B. Han*, G. Niu, T. Liu, and M. Sugiyama.<br>
                Where is the bottleneck of adversarial learning with unlabeled data?<br>
                [ <a href="http://arxiv.org/abs/1911.08696" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Y. Pan, W. Chen, G. Niu, I. W. Tsang, and M. Sugiyama.<br>
                Fast and robust rank aggregation against model misspecification.<br>
                [ <a href="http://arxiv.org/abs/1905.12341" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                F. Liu, J. Lu, B. Han, G. Niu, G. Zhang, and M. Sugiyama.<br>
                Butterfly: A panacea for all difficulties in wildly unsupervised domain adaptation.<br>
                [ <a href="http://arxiv.org/abs/1905.07720" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                C.-Y. Hsieh, M. Xu, G. Niu, H.-T. Lin, and M. Sugiyama.<br>
                A pseudo-label method for coarse-to-fine multi-label learning with limited supervision.<br>
                [ <a href="http://openreview.net/forum?id=rylVYjqHdN" target="_blank">OpenReview</a> ]
            </p></li>
            <li><p>
                M. Xu, B. Li, G. Niu, B. Han, and M. Sugiyama.<br>
                Revisiting sample selection approach to positive-unlabeled learning: Turning unlabeled data into positive rather than negative.<br>
                [ <a href="http://arxiv.org/abs/1901.10155" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                M. Kato, L. Xu, G. Niu, and M. Sugiyama.<br>
                Alternate estimation of a classifier and the class-prior from positive and unlabeled data.<br>
                [ <a href="http://arxiv.org/abs/1809.05710" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                M. Xu, G. Niu, B. Han, I. W. Tsang, Z.-H. Zhou, and M. Sugiyama.<br>
                Matrix co-completion for multi-label classification with missing features and labels.<br>
                [ <a href="http://arxiv.org/abs/1805.09156" target="_blank">arXiv</a> ]
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="conference"></a>Conference Papers (full review)</h2>
        <ol>
            <li><p>
                L. Feng, S. Shu, Y. Cao, L. Tao, H. Wei, T. Xiang, B. An, and G. Niu.<br>
                Multiple-instance learning from similar and dissimilar bags.<br>
                In <i>Proceedings of <a href="http://www.kdd.org/kdd2021/" target="_blank">27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2021)</a></i>,
                to appear.
            </p></li>
            <li><p>
                S. Chen, G. Niu, C. Gong, J. Li, J. Yang, and M. Sugiyama.<br>
                Large-margin contrastive learning with distance polarization regularizer.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 1673--1683, Online, Jul 18--24, 2021.
            </p></li>
            <li><p>
                H. Yan, J. Zhang, G. Niu, J. Feng, V. Y. F. Tan, and M. Sugiyama.<br>
                CIFS: Improving adversarial robustness of CNNs via channel-wise importance-based feature selection.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 11693--11703, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2102.05311" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                R. Gao*, F. Liu*, J. Zhang*, B. Han, T. Liu, G. Niu, and M. Sugiyama.<br>
                Maximum mean discrepancy test is aware of adversarial attacks.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 3564--3575, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2010.11415" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                X. Li, T. Liu, B. Han, G. Niu, and M. Sugiyama.<br>
                Provably end-to-end label-noise learning without anchor points.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 6403--6413, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2102.02400" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                X. Du*, J. Zhang*, B. Han, T. Liu, Y. Rong, G. Niu, J. Huang, and M. Sugiyama.<br>
                Learning diverse-structured networks for adversarial robustness.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 2880--2891, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2102.01886" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                A. Berthon, B. Han, G. Niu, T. Liu, and M. Sugiyama.<br>
                Confidence scores make instance-dependent label-noise learning possible.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 825--836, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2001.03772" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Y. Zhang, G. Niu, and M. Sugiyama.<br>
                Learning noise transition matrix from only noisy labels via total variation regularization.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 12501--12512, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2102.02414" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                L. Feng, S. Shu, N. Lu, B. Han, M. Xu, G. Niu, B. An, and M. Sugiyama.<br>
                Pointwise binary classification with pairwise confidence comparisons.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 3252--3262, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2010.01875" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                N. Lu*, S. Lei*, G. Niu, I. Sato, and M. Sugiyama.<br>
                Binary classification from multiple unlabeled datasets via surrogate set classification.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 7134--7144, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2102.00678" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Y. Cao, L. Feng, Y. Xu, B. An, G. Niu, and M. Sugiyama.<br>
                Learning from similarity-confidence data.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 1272--1282, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2102.06879" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                S. Wu*, X. Xia*, T. Liu, B. Han, M. Gong, N. Wang, H. Liu, and G. Niu.<br>
                Class2Simi: A noise reduction perspective on learning with noisy labels.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2021" target="_blank">38th International Conference on Machine Learning (ICML 2021)</a></i>,
                PMLR, vol. 139, pp. 11285--11295, Online, Jul 18--24, 2021.<br>
                [ <a href="http://arxiv.org/abs/2006.07831" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Zhang, J. Zhu, G. Niu, B. Han, M. Sugiyama, and M. Kankanhalli.<br>
                Geometry-aware instance-reweighted adversarial training.<br>
                In <i>Proceedings of <a href="http://iclr.cc/Conferences/2021/" target="_blank">9th International Conference on Learning Representations (ICLR 2021)</a></i>,
                29 pages, Online, May 3--7, 2021.<br>
                <font color="#008800">(This paper was selected for oral presentation;
                orals : acceptance : submissions = 53 : 860 : 2997)</font><br>
                [ <a href="http://arxiv.org/abs/2010.01736" target="_blank">arXiv</a>,
                <a href="http://openreview.net/forum?id=iAX0l6Cz8ub" target="_blank">OpenReview</a> ]
            </p></li>
            <li><p>
                A. Jacovi, G. Niu, Y. Goldberg, and M. Sugiyama.<br>
                Scalable evaluation and improvement of document set expansion via neural positive-unlabeled learning.<br>
                In <i>Proceedings of <a href="http://2021.eacl.org/" target="_blank">16th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2021)</a></i>,
                to appear.<br>
                [ <a href="http://arxiv.org/abs/1910.13339" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Q. Wang, B. Han, T. Liu, G. Niu, J. Yang, and C. Gong.<br>
                Tackling instance-dependent label noise via a universal probabilistic model.<br>
                In <i>Proceedings of <a href="http://aaai.org/Conferences/AAAI-21/" target="_blank">35th AAAI Conference on Artificial Intelligence (AAAI 2021)</a></i>,
                to appear.<br>
                [ <a href="paper/wang_aaai21.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                X. Xia, T. Liu, B. Han, N. Wang, M. Gong, H. Liu, G. Niu, D. Tao, and M. Sugiyama.<br>
                Part-dependent label noise: Towards instance-dependent label noise.<br>
                In <i><a href="http://neurips.cc/Conferences/2020/" target="_blank">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a></i>,
                pp. 7597--7610, Online, Dec 6--12, 2020.<br>
                <font color="#008800">(This paper was selected for spotlight presentation;
                spotlights : acceptance : submissions = 280 : 1900 : 9454)</font><br>
                [ <a href="paper/xia_neurips20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Fang*, N. Lu*, G. Niu, and M. Sugiyama.<br>
                Rethinking importance weighting for deep learning under distribution shift.<br>
                In <i><a href="http://neurips.cc/Conferences/2020/" target="_blank">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a></i>,
                pp. 11996--12007, Online, Dec 6--12, 2020.<br>
                <font color="#008800">(This paper was selected for spotlight presentation;
                spotlights : acceptance : submissions = 280 : 1900 : 9454)</font><br>
                [ <a href="paper/fang_neurips20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                Y. Yao, T. Liu, B. Han, M. Gong, J. Deng, G. Niu, and M. Sugiyama.<br>
                Dual T: Reducing estimation error for transition matrix in label-noise learning.<br>
                In <i><a href="http://neurips.cc/Conferences/2020/" target="_blank">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a></i>,
                pp. 7260--7271, Online, Dec 6--12, 2020.<br>
                [ <a href="paper/yao_neurips20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                L. Feng, J. Lv, B. Han, M. Xu, G. Niu, X. Geng, B. An, and M. Sugiyama.<br>
                Provably consistent partial-label learning.<br>
                In <i><a href="http://neurips.cc/Conferences/2020/" target="_blank">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a></i>,
                pp. 10948--10960, Online, Dec 6--12, 2020.<br>
                [ <a href="paper/feng_neurips20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                C. Wang, B. Han, S. Pan, J. Jiang, G. Niu, and G. Long.<br>
                Cross-graph: Robust and unsupervised embedding for attributed graphs with corrupted structure.<br>
                In <i>Proceedings of <a href="http://icdm2020.bigke.org/" target="_blank">20th IEEE International Conference on Data Mining (ICDM 2020)</a></i>,
                pp. 571-580, Online, Nov 17--20, 2020.<br>
                [ <a href="paper/wang_icdm20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                J. Zhang*, X. Xu*, B. Han, G. Niu, L. Cui, M. Sugiyama, and M. Kankanhalli.<br>
                Attacks which do not kill training make adversarial learning stronger.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2020" target="_blank">37th International Conference on Machine Learning (ICML 2020)</a></i>,
                PMLR, vol. 119, pp. 11278--11287, Online, Jul 12--18, 2020.<br>
                [ <a href="paper/zhang_icml20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Han, G. Niu, X. Yu, Q. Yao, M. Xu, I. W. Tsang, and M. Sugiyama.<br>
                SIGUA: Forgetting may make learning with noisy labels more robust.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2020" target="_blank">37th International Conference on Machine Learning (ICML 2020)</a></i>,
                PMLR, vol. 119, pp. 4006--4016, Online, Jul 12--18, 2020.<br>
                [ <a href="paper/han_icml20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                L. Feng*, T. Kaneko*, B. Han, G. Niu, B. An, and M. Sugiyama.<br>
                Learning with multiple complementary labels.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2020" target="_blank">37th International Conference on Machine Learning (ICML 2020)</a></i>,
                PMLR, vol. 119, pp. 3072--3081, Online, Jul 12--18, 2020.<br>
                [ <a href="paper/feng_icml20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                Y.-T. Chou, G. Niu, H.-T. Lin, and M. Sugiyama.<br>
                Unbiased risk estimators can mislead: A case study of learning with complementary labels.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2020" target="_blank">37th International Conference on Machine Learning (ICML 2020)</a></i>,
                PMLR, vol. 119, pp. 1929--1938, Online, Jul 12--18, 2020.<br>
                [ <a href="paper/chou_icml20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                Q. Yao, H. Yang, B. Han, G. Niu, and J. T. Kwok.<br>
                Searching to exploit memorization effect in learning with noisy labels.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2020" target="_blank">37th International Conference on Machine Learning (ICML 2020)</a></i>,
                PMLR, vol. 119, pp. 10789--10798, Online, Jul 12--18, 2020.<br>
                [ <a href="paper/yao_icml20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                J. Lv, M. Xu, L. Feng, G. Niu, X. Geng, and M. Sugiyama<br>
                Progressive identification of true labels for partial-label learning.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2020" target="_blank">37th International Conference on Machine Learning (ICML 2020)</a></i>,
                PMLR, vol. 119, pp. 6500--6510, Online, Jul 12--18, 2020.<br>
                [ <a href="paper/lv_icml20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Ishida, I. Yamane, T. Sakai, G. Niu, and M. Sugiyama.<br>
                Do we need zero training loss after achieving zero training error?<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2020" target="_blank">37th International Conference on Machine Learning (ICML 2020)</a></i>,
                PMLR, vol. 119, pp. 4604--4614, Online, Jul 12--18, 2020.<br>
                [ <a href="paper/ishida_icml20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                N. Lu, T. Zhang, G. Niu, and M. Sugiyama.<br>
                Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach.<br>
                In <i>Proceedings of <a href="http://aistats.org/aistats2020/" target="_blank">23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)</a></i>,
                PMLR, vol. 108, pp. 1115--1125, Online, Aug 26--28, 2020.<br>
                [ <a href="paper/lu_aistats20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                C. Li, M. E. Khan, Z. Sun, G. Niu, B. Han, S. Xie, and Q. Zhao.<br>
                Beyond unfolding: Exact recovery of latent convex tensor decomposition under reshuffling.<br>
                In <i>Proceedings of <a href="http://aaai.org/Conferences/AAAI-20/" target="_blank">34th AAAI Conference on Artificial Intelligence (AAAI 2020)</a></i>,
                pp. 4602--4609, New York, New York, USA, Feb 7--12, 2020.<br>
                [ <a href="paper/li_aaai20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                L. Xu, J. Honda, G. Niu, and M. Sugiyama.<br>
                Uncoupled regression from pairwise comparison data.<br>
                In <i><a href="http://neurips.cc/Conferences/2019/" target="_blank">Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</a></i>,
                pp. 3992--4002, Vancouver, British Columbia, Canada, Dec 8--14, 2019.<br>
                [ <a href="paper/xu_neurips19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, and M. Sugiyama.<br>
                Are anchor points really indispensable in label-noise learning?<br>
                In <i><a href="http://neurips.cc/Conferences/2019/" target="_blank">Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</a></i>,
                pp. 6838--6849, Vancouver, British Columbia, Canada, Dec 8--14, 2019.<br>
                [ <a href="paper/xia_neurips19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                Y.-G. Hsieh, G. Niu, and M. Sugiyama.<br>
                Classification from positive, unlabeled and biased negative data.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2019" target="_blank">36th International Conference on Machine Learning (ICML 2019)</a></i>,
                PMLR, vol. 97, pp. 2820--2829, Long Beach, California, USA, Jun 9--15, 2019.<br>
                [ <a href="paper/hsieh_icml19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Ishida, G. Niu, A. K. Menon, and M. Sugiyama.<br>
                Complementary-label learning for arbitrary losses and models.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2019" target="_blank">36th International Conference on Machine Learning (ICML 2019)</a></i>,
                PMLR, vol. 97, pp. 2971--2980, Long Beach, California, USA, Jun 9--15, 2019.<br>
                [ <a href="paper/ishida_icml19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                X. Yu, B. Han, J. Yao, G. Niu, I. W. Tsang, and M. Sugiyama.<br>
                How does disagreement help generalization against label corruption?<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2019" target="_blank">36th International Conference on Machine Learning (ICML 2019)</a></i>,
                PMLR, vol. 97, pp. 7164--7173, Long Beach, California, USA, Jun 9--15, 2019.<br>
                [ <a href="paper/yu_icml19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                N. Lu, G. Niu, A. K. Menon, and M. Sugiyama.<br>
                On the minimal supervision for training any binary classifier from only unlabeled data.<br>
                In <i>Proceedings of <a href="http://iclr.cc/Conferences/2019/" target="_blank">7th International Conference on Learning Representations (ICLR 2019)</a></i>,
                18 pages, New Orleans, Louisiana, USA, May 6--9, 2019.<br>
                [ <a href="paper/lu_iclr19.pdf" target="_blank">paper</a>,
                <a href="http://openreview.net/forum?id=B1xWcj0qYm" target="_blank">OpenReview</a> ]
            </p></li>
            <li><p>
                T. Ishida, G. Niu, and M. Sugiyama.<br>
                Binary classification from positive-confidence data.<br>
                In <i><a href="http://neurips.cc/Conferences/2018/" target="_blank">Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</a></i>,
                pp. 5917--5928, Montreal, Quebec, Canada, Dec 2--8, 2018.<br>
                <font color="#008800">(This paper was selected for spotlight presentation;
                spotlights : acceptance : submissions = 168 : 1011 : 4856)</font><br>
                [ <a href="paper/ishida_neurips18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Han*, J. Yao*, G. Niu, M. Zhou, I. W. Tsang, Y. Zhang, and M. Sugiyama.<br>
                Masking: A new perspective of noisy supervision.<br>
                In <i><a href="http://neurips.cc/Conferences/2018/" target="_blank">Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</a></i>,
                pp. 5836--5846, Montreal, Quebec, Canada, Dec 2--8, 2018.<br>
                [ <a href="paper/han_neurips18a.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Han*, Q. Yao*, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama.<br>
                Co-teaching: Robust training of deep neural networks with extremely noisy labels.<br>
                In <i><a href="http://neurips.cc/Conferences/2018/" target="_blank">Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</a></i>,
                pp. 8527--8537, Montreal, Quebec, Canada, Dec 2--8, 2018.<br>
                [ <a href="paper/han_neurips18b.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                W. Hu, G. Niu, I. Sato, and M. Sugiyama.<br>
                Does distributionally robust supervised learning give robust classifiers?<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2018" target="_blank">35th International Conference on Machine Learning (ICML 2018)</a></i>,
                PMLR, vol. 80, pp. 2029--2037, Stockholm, Sweden, Jul 10--15, 2018.<br>
                [ <a href="paper/hu_icml18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                H. Bao, G. Niu, and M. Sugiyama.<br>
                Classification from pairwise similarity and unlabeled data.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2018" target="_blank">35th International Conference on Machine Learning (ICML 2018)</a></i>,
                PMLR, vol. 80, pp. 452--461, Stockholm, Sweden, Jul 10--15, 2018.<br>
                [ <a href="paper/bao_icml18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                S.-J. Huang, M. Xu, M.-K. Xie, M. Sugiyama, G. Niu, and S. Chen.<br>
                Active feature acquisition with supervised matrix completion.<br>
                In <i>Proceedings of <a href="http://www.kdd.org/kdd2018/" target="_blank">24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2018)</a></i>,
                pp. 1571--1579, London, UK, Aug 19--23, 2018.<br>
                [ <a href="paper/huang_kdd18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                R. Kiryo, G. Niu, M. C. du Plessis, and M. Sugiyama.<br>
                Positive-unlabeled learning with non-negative risk estimator.<br>
                In <i><a href="http://neurips.cc/Conferences/2017/" target="_blank">Advances in Neural Information Processing Systems 30 (NeurIPS 2017)</a></i>,
                pp. 1674--1684, Long Beach, California, USA, Dec 4--9, 2017.<br>
                <font color="#008800">(This paper was selected for oral presentation;
                orals : acceptance : submissions = 40 : 678 : 3240)</font><br>
                [ <a href="paper/kiryo_nips17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Ishida, G. Niu, W. Hu, and M. Sugiyama.<br>
                Learning from complementary labels.<br>
                In <i><a href="http://neurips.cc/Conferences/2017/" target="_blank">Advances in Neural Information Processing Systems 30 (NeurIPS 2017)</a></i>,
                pp. 5644--5654, Long Beach, California, USA, Dec 4--9, 2017.<br>
                [ <a href="paper/ishida_nips17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                H. Shiino, H. Sasaki, G. Niu, and M. Sugiyama.<br>
                Whitening-free least-squares non-Gaussian component analysis.<br>
                In <i>Proceedings of <a href="http://acml-conf.org/2017/" target="_blank">9th Asian Conference on Machine Learning (ACML 2017)</a></i>,
                PMLR, vol. 77, pp. 375--390, Seoul, Korea, Nov 15--17, 2017.<br>
                <font color="#008800">(This paper was selected for Best Paper Runner-up Award)</font><br>
                [ <a href="paper/shiino_acml17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Sakai, M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Semi-supervised classification based on classification from positive and unlabeled data.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2017" target="_blank">34th International Conference on Machine Learning (ICML 2017)</a></i>,
                PMLR, vol. 70, pp. 2998--3006, Sydney, Australia, Aug 6--11, 2017.<br>
                [ <a href="paper/sakai_icml17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, M. C. du Plessis, T. Sakai, Y. Ma, and M. Sugiyama.<br>
                Theoretical comparisons of positive-unlabeled learning against positive-negative learning.<br>
                In <i><a href="http://neurips.cc/Conferences/2016/" target="_blank">Advances in Neural Information Processing Systems 29 (NeurIPS 2016)</a></i>,
                pp. 1199--1207, Barcelona, Spain, Dec 5--10, 2016.<br>
                [ <a href="paper/niu_nips16.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                H. Sasaki, G. Niu, and M. Sugiyama.<br>
                Non-Gaussian component analysis with log-density gradient estimation.<br>
                In <i>Proceedings of <a href="http://aistats.org/aistats2016/" target="_blank">19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016)</a></i>,
                PMLR, vol. 51, pp. 1177--1185, Cadiz, Spain, May 9--11, 2016.<br>
                [ <a href="paper/sasaki_aistats16.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Zhao, G. Niu, N. Xie, J. Yang, and M. Sugiyama.<br>
                Regularized policy gradients: Direct variance reduction in policy gradient estimation.<br>
                In <i>Proceedings of <a href="http://acml-conf.org/2015/" target="_blank">7th Asian Conference on Machine Learning (ACML 2015)</a></i>,
                PMLR, vol. 45, pp. 333--348, Hong Kong, China, Nov 20--22, 2015.<br>
                [ <a href="paper/zhao_acml15.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Class-prior estimation for learning from positive and unlabeled data.<br>
                In <i>Proceedings of <a href="http://acml-conf.org/2015/" target="_blank">7th Asian Conference on Machine Learning (ACML 2015)</a></i>,
                PMLR, vol. 45, pp. 221--236, Hong Kong, China, Nov 20--22, 2015.<br>
                [ <a href="paper/duplessis_acml15.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Convex formulation for learning from positive and unlabeled data.<br>
                In <i>Proceedings of <a href="http://icml.cc/2015/" target="_blank">32nd International Conference on Machine Learning (ICML 2015)</a></i>,
                PMLR, vol. 37, pp. 1386--1394, Lille, France, Jul 6--11, 2015.<br>
                [ <a href="paper/duplessis_icml15.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Analysis of learning from positive and unlabeled data.<br>
                In <i><a href="http://neurips.cc/Conferences/2014/" target="_blank">Advances in Neural Information Processing Systems 27 (NeurIPS 2014)</a></i>,
                pp. 703--711, Montreal, Quebec, Canada, Dec 8--13, 2014.<br>
                [ <a href="paper/duplessis_nips14.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. C. du Plessis, and M. Sugiyama.<br>
                Transductive learning with multi-class volume approximation.<br>
                In <i>Proceedings of <a href="http://icml.cc/2014/" target="_blank">31st International Conference on Machine Learning (ICML 2014)</a></i>,
                PMLR, vol. 32, no. 2, pp. 1377--1385, Beijing, China, Jun 21--26, 2014.<br>
                [ <a href="paper/niu_icml14.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Clustering unclustered data: Unsupervised binary labeling of two datasets having different class balances.<br>
                In <i>Proceedings of 2013 Conference on Technologies and Applications of Artificial Intelligence (TAAI 2013)</i>,
                pp. 1--6, Taipei, Taiwan, Dec 6--8, 2013.<br>
                <font color="#008800">(This paper was selected for Best Paper Award)</font><br>
                [ <a href="paper/duplessis_taai13.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, W. Jitkrittum, B. Dai, H. Hachiya, and M. Sugiyama.<br>
                Squared-loss mutual information regularization: A novel information-theoretic approach to semi-supervised learning.<br>
                In <i>Proceedings of <a href="http://icml.cc/2013/" target="_blank">30th International Conference on Machine Learning (ICML 2013)</a></i>,
                PMLR, vol. 28, no. 3, pp. 10--18, Atlanta, Georgia, USA, Jun 16--21, 2013.<br>
                [ <a href="paper/niu_icml13.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. Yamada, and M. Sugiyama.<br>
                Information-theoretic semi-supervised metric learning via entropy regularization.<br>
                In <i>Proceedings of <a href="http://icml.cc/2012/" target="_blank">29th International Conference on Machine Learning (ICML 2012)</a></i>,
                pp. 89--96, Edinburgh, Scotland, Jun 26--Jul 1, 2012.<br>
                [ <a href="paper/niu_icml12.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Zhao, H. Hachiya, G. Niu, and M. Sugiyama.<br>
                Analysis and improvement of policy gradient estimation.<br>
                In <i><a href="http://neurips.cc/Conferences/2011/" target="_blank">Advances in Neural Information Processing Systems 24 (NeurIPS 2011)</a></i>,
                pp. 262--270, Granada, Spain, Dec 12--17, 2011.<br>
                [ <a href="paper/zhao_nips11.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. Yamada, G. Niu, J. Takagi, and M. Sugiyama.<br>
                Computationally efficient sufficient dimension reduction via squared-loss mutual information.<br>
                In <i>Proceedings of 3rd Asian Conference on Machine Learning (ACML 2011)</i>,
                PMLR, vol. 20, pp. 247--262, Taoyuan, Taiwan, Nov 13--15, 2011.<br>
                [ <a href="paper/yamada_acml11.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, L. Shang, and M. Sugiyama.<br>
                Maximum volume clustering.<br>
                In <i>Proceedings of <a href="http://aistats.org/aistats2011/" target="_blank">14th International Conference on Artificial Intelligence and Statistics (AISTATS 2011)</a></i>,
                PMLR, vol. 15, pp. 561--569, Fort Lauderdale, Florida, USA, Apr 11--13, 2011.<br>
                [ <a href="paper/niu_aistats11.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Dai, B. Hu, and G. Niu.<br>
                Bayesian maximum margin clustering.<br>
                In <i>Proceedings of 10th IEEE International Conference on Data Mining (ICDM 2010)</i>,
                pp. 108--117, Sydney, Australia, Dec 14--17, 2010.<br>
                [ <a href="paper/dai_icdm10.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, Y. Ji, and L. Shang.<br>
                Rough margin based core vector machine.<br>
                In <i>Proceedings of 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2010)</i>,
                LNCS, vol. 6118, pp. 134--141, Hyderabad, India, Jun 21--24, 2010.<br>
                [ <a href="paper/niu_pakdd10.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Dai and G. Niu.<br>
                Compact margin machine.<br>
                In <i>Proceedings of 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2010)</i>,
                LNCS, vol. 6119, pp. 507--514, Hyderabad, India, Jun 21--24, 2010.<br>
                [ <a href="paper/dai_pakdd10.pdf" target="_blank">paper</a> ]
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="journal"></a>Journal Articles</h2>
        <ol>
            <li><p>
                W. Xu, G. Niu, A. Hyv√§rinen, and M. Sugiyama.<br>
                Direction matters: On influence-preserving graph summarization and max-cut principle for directed graphs.<br>
                <i><a href="http://direct.mit.edu/neco" target="_blank">Neural Computation</a></i>, to appear.<br>
                [ <a href="http://doi.org/10.1162/neco_a_01402" target="_blank">link</a> ]
            </p></li>
            <li><p>
                T. Sakai, G. Niu, and M. Sugiyama.<br>
                Information-theoretic representation learning for positive-unlabeled classification.<br>
                <i><a href="http://direct.mit.edu/neco" target="_blank">Neural Computation</a></i>, vol. 33, no. 1, pp. 244--268, 2021.<br>
                [ <a href="http://doi.org/10.1162/neco_a_01337" target="_blank">link</a> ]
            </p></li>
            <li><p>
                H. Sasaki, T. Kanamori, A. Hyv√§rinen, G. Niu, and M. Sugiyama.<br>
                Mode-seeking clustering and density ridge estimation via direct estimation of density-derivative-ratios.<br>
                <i><a href="http://jmlr.org/" target="_blank">Journal of Machine Learning Research</a></i>, vol. 18, no. 180, pp. 1--45, 2018.<br>
                [ <a href="http://jmlr.org/papers/v18/17-380.html"  target="_blank">link</a> ]
            </p></li>
            <li><p>
                T. Sakai, G. Niu, and M. Sugiyama.<br>
                Semi-supervised AUC optimization based on positive-unlabeled learning.<br>
                <i><a href="http://www.springer.com/journal/10994" target="_blank">Machine Learning</a></i>, vol. 107, no. 4, pp. 767--794, 2018.<br>
                [ <a href="http://doi.org/10.1007/s10994-017-5678-9" target="_blank">link</a> ]
            </p></li>
            <li><p>
                H. Sasaki, V. Tangkaratt, G. Niu, and M. Sugiyama.<br>
                Sufficient dimension reduction via direct estimation of the gradients of logarithmic conditional densities.<br>
                <i><a href="http://direct.mit.edu/neco" target="_blank">Neural Computation</a></i>, vol. 30, no. 2, pp. 477--504, 2018.<br>
                [ <a href="http://doi.org/10.1162/neco_a_01035" target="_blank">link</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis*, G. Niu*, and M. Sugiyama.<br>
                Class-prior estimation for learning from positive and unlabeled data.<br>
                <i><a href="http://www.springer.com/journal/10994" target="_blank">Machine Learning</a></i>, vol. 106, no. 4, pp. 463--492, 2017.<br>
                [ <a href="http://doi.org/10.1007/s10994-016-5604-6" target="_blank">link</a> ]
            </p></li>
            <li><p>
                H. Sasaki, Y.-K. Noh, G. Niu, and M. Sugiyama.<br>
                Direct density-derivative estimation.<br>
                <i><a href="http://direct.mit.edu/neco" target="_blank">Neural Computation</a></i>,  vol. 28, no. 6, pp. 1101--1140, 2016.<br>
                [ <a href="http://doi.org/10.1162/neco_a_00835" target="_blank">link</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. Yamada, and M. Sugiyama.<br>
                Information-theoretic semi-supervised metric learning via entropy regularization.<br>
                <i><a href="http://direct.mit.edu/neco" target="_blank">Neural Computation</a></i>,  vol. 26, no. 8, pp. 1717--1762, 2014.<br>
                [ <a href="http://doi.org/10.1162/neco_a_00614" target="_blank">link</a> ]
            </p></li>
            <li><p>
                D. Calandriello, G. Niu, and M. Sugiyama.<br>
                Semi-supervised information-maximization clustering.<br>
                <i><a href="http://www.journals.elsevier.com/neural-networks" target="_blank">Neural Networks</a></i>, vol. 57, pp. 103--111, 2014.<br>
                [ <a href="http://doi.org/10.1016/j.neunet.2014.05.016" target="_blank">link</a> ]
            </p></li>
            <li><p>
                M. Sugiyama, G. Niu, M. Yamada, M. Kimura, and H. Hachiya.<br>
                Information-maximization clustering based on squared-loss mutual information.<br>
                <i><a href="http://direct.mit.edu/neco" target="_blank">Neural Computation</a></i>, vol. 26, no. 1, pp. 84--131, 2014.<br>
                [ <a href="http://doi.org/10.1162/neco_a_00534" target="_blank">link</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, L. Shang, and M. Sugiyama.<br>
                Maximum volume clustering: A new discriminative clustering approach.<br>
                <i><a href="http://jmlr.org/" target="_blank">Journal of Machine Learning Research</a></i>, vol. 14 (Sep), pp. 2641--2687, 2013.<br>
                [ <a href="http://jmlr.org/papers/v14/niu13a.html" target="_blank">link</a> ]
            </p></li>
            <li><p>
                T. Zhao, H. Hachiya, G. Niu, and M. Sugiyama.<br>
                Analysis and improvement of policy gradient estimation.<br>
                <i><a href="http://www.journals.elsevier.com/neural-networks" target="_blank">Neural Networks</a></i>, vol. 26, pp. 118--129, 2012.<br>
                [ <a href="http://doi.org/10.1016/j.neunet.2011.09.005" target="_blank">link</a> ]
            </p></li>
            <li><p>
                Y. Ji, J. Chen, G. Niu, L. Shang, and X. Dai.<br>
                Transfer learning via multi-view principal component analysis.<br>
                <i><a href="http://www.springer.com/journal/11390" target="_blank">Journal of Computer Science and Technology</a></i>, vol. 26, no. 1, pp. 81--98, 2011.<br>
                [ <a href="http://doi.org/10.1007/s11390-011-9417-6" target="_blank">link</a> ]
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="workshop"></a>Workshop Presentations (selected)</h2>
        <ol>
            <li><p>
                G. Niu.<br>
                Robust learning against label noise.<br>
                Presented at <i><a href="http://indico2.riken.jp/event/3146/" target="_blank">The All-RIKEN Workshop 2019</a></i>, Wako, Japan, Dec 5--6, 2019.<br>
                <font color="#008800">(This was an award speech)</font><br>
            </p></li>
            <li><p>
                G. Niu.<br>
                When weakly supervised learning meets deep learning.<br>
                Presented at <i><a href="http://www.ijcai-boom.org/" target="_blank">3rd IJCAI BOOM</a></i>, Stockholm, Sweden, Jul 13, 2018.<br>
                <font color="#008800">(This was an invited talk)</font><br>
            </p></li>
            <li><p>
                G. Niu.<br>
                When deep learning meets weakly supervised learning.<br>
                Presented at <i><a href="http://www.ms.k.u-tokyo.ac.jp/TDLW2018/" target="_blank">Deep Learning: Theory, Algorithms, and Applications</a></i>, Tokyo, Japan, Mar 19--22, 2018.<br>
                <font color="#008800">(This was an invited talk)</font><br>
                [ <a href="paper/niu_tdlw2018.pdf" target="_blank">slides</a>,
                <a href="http://www.youtube.com/watch?v=onGqIZ1SHs0" target="_blank">video</a> ]
            </p></li>
            <li><p>
                G. Niu.<br>
                Statistical learning from weak supervision.<br>
                Presented at <i><a href="http://ircn.jp/en/archives/6603" target="_blank">1st IRCN Retreat 2018</a></i>, Yokohama, Japan, Mar 17--18, 2018.<br>
                <font color="#008800">(This was an invited talk)</font>
            </p></li>
            <li><p>
                G. Niu.<br>
                Recent advances on positive-unlabeled (PU) learning.<br>
                Presented at <i><a href="http://ibisml.org/ibisml030" target="_blank">30th IBISML (joint with PRMU and CVIM)</a></i>, Tokyo, Japan, Sep 15--16, 2017.<br>
                <font color="#008800">(This was an invited talk)</font><br>
                [ <a href="paper/niu_ibisml030.pdf" target="_blank">slides</a> ]
            </p></li>
            <li><p>
                G. Niu (presented by Tomoya Sakai).<br>
                Positive-unlabeled learning with application to semi-supervised learning.<br>
                Presented at <i><a href="http://www.microsoft.com/en-us/research/event/microsoft-research-asia-academic-day-2017/" target="_blank">Microsoft Research Asia Academic Day 2017</a></i>, Yilan, Taiwan, May 26, 2017.
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. Yamada, and M. Sugiyama.<br>
                Information-theoretic semi-supervised metric learning via entropy regularization.<br>
                Presented at <i><a href="http://www.iip.ist.i.kyoto-u.ac.jp/mlss12/doku.php" target="_blank">21st MLSS</a></i>, Kyoto, Japan, Aug 27--Sep 7, 2012.
            </p></li>
            <li><p>
                G. Niu, B. Dai, L. Shang, and M. Sugiyama.<br>
                Maximum volume clustering.<br>
                Presented at <i><a href="http://mlss11.bordeaux.inria.fr/" target="_blank">18th MLSS</a></i>, Bordeaux, France, Sep 4--17, 2011.
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="thesis"></a>Theses</h2>
        <ol>
            <li><p>
                Gang Niu.<br>
                <i>Discriminative methods with imperfect supervision in machine learning</i> (204 pages).<br>
                Doctoral Thesis, Department of Computer Science, Tokyo Institute of Technology, Sep 2013.
            </p></li>
            <li><p>
                Gang Niu.<br>
                <i>Support vector learning based on rough set modeling</i> (71 pages in Chinese).<br>
                Master Thesis, Department of Computer Science and Technology, Nanjing University, May 2010.
            </p></li>
        </ol>
    </div>
